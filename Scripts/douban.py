"""
site: douban.com
    description: å°†ä»è±†ç“£è¯»ä¹¦å¯¼å‡ºçš„HTMLæ ¼å¼è®ºæ–‡è½¬æ¢ä¸ºç¬¦åˆPKBè§„èŒƒçš„Markdownæ–‡ä»¶
"""


import re
from bs4 import BeautifulSoup

def parse_douban_html(html_content):
    """
    è§£æè±†ç“£è¯»ä¹¦é¡µé¢çš„HTMLï¼Œæå–ä¹¦ç±ä¿¡æ¯å¹¶ç”ŸæˆMarkdownæ ¼å¼çš„ç¬”è®°ã€‚
    """
    soup = BeautifulSoup(html_content, 'lxml')
    
    # --- 1. æå–ä¹¦ç±å…ƒæ•°æ® ---
    info_div = soup.find('div', id='info')
    if not info_div:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä¹¦ç±ä¿¡æ¯åŒºåŸŸ (div#info)ã€‚")
        return None

    # è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå®‰å…¨åœ°æå–ä¿¡æ¯
    def get_info_text(label):
        tag = info_div.find('span', string=re.compile(label))
        if tag:
            # ä¼˜å…ˆæŸ¥æ‰¾aæ ‡ç­¾
            a_tag = tag.find_next('a')
            if a_tag and a_tag.parent == tag.parent:
                return a_tag.get_text(strip=True)
            # .next_sibling å¯èƒ½ä¼šæ˜¯æ¢è¡Œç¬¦æˆ–ç©ºæ ¼ï¼Œéœ€è¦å¾ªç¯æŸ¥æ‰¾ä¸‹ä¸€ä¸ªæœ‰æ•ˆèŠ‚ç‚¹
            next_s = tag.next_sibling
            while next_s and isinstance(next_s, str) and not next_s.strip():
                next_s = next_s.next_sibling
            if next_s:
                return next_s.get_text(strip=True) if getattr(next_s, 'name', None) == 'a' else next_s.strip()
        return None
        
    def get_all_info_texts(label):
        tags = info_div.find_all('span', string=re.compile(label))
        if tags:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªåŒ¹é…çš„æ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ä½œè€…/è¯‘è€…ï¼‰
            tag = tags[-1]
            # æŸ¥æ‰¾è¯¥æ ‡ç­¾åçš„æ‰€æœ‰ a æ ‡ç­¾å…„å¼ŸèŠ‚ç‚¹
            translators = [a.get_text(strip=True) for a in tag.find_next_siblings('a')]
            return translators
        return []

    title = soup.find('span', property='v:itemreviewed').get_text(strip=True)
    author = get_info_text('ä½œè€…')
    publisher = get_info_text('å‡ºç‰ˆç¤¾')
    subtitle = get_info_text('å‰¯æ ‡é¢˜')
    producer = get_info_text('å‡ºå“æ–¹') # æœ‰äº›ä¹¦æœ‰å‡ºå“æ–¹
    series = get_info_text('ä¸›ä¹¦')
    original_title = get_info_text('åŸä½œå')
    translators = get_all_info_texts('è¯‘è€…')
    publish_date = get_info_text('å‡ºç‰ˆå¹´')
    isbn = get_info_text('ISBN')
    
    # å°é¢å›¾ç‰‡é“¾æ¥ï¼ˆå–imgçš„srcï¼‰
    cover_tag = soup.find('a', class_='nbg')
    cover_url = ''
    if cover_tag:
        img_tag = cover_tag.find('img')
        if img_tag and img_tag.has_attr('src'):
            cover_url = img_tag['src']

    # --- 2. æå–ç›®å½• ---
    # ç›®å½•çš„div idæ˜¯åŠ¨æ€çš„ï¼Œä½†é€šå¸¸ä»¥'dir_'å¼€å¤´ï¼Œä»¥'_full'ç»“å°¾
    toc_div = soup.find('div', id=lambda x: x and x.startswith('dir_') and x.endswith('_full'))
    toc_markdown = ""
    if toc_div:
        # ä½¿ç”¨get_textå¹¶æŒ‡å®šåˆ†éš”ç¬¦ï¼Œå°†<br>æ›¿æ¢ä¸ºæ¢è¡Œ
        toc_text = toc_div.get_text(separator='\n', strip=True)
        toc_lines = toc_text.splitlines()
        # è½¬æ¢æˆMarkdownä»»åŠ¡åˆ—è¡¨æ ¼å¼
        toc_markdown_list = [f"- [ ] {line.strip()}" for line in toc_lines if line.strip()]
        toc_markdown = '\n'.join(toc_markdown_list)
    else:
        toc_markdown = "# ç›®å½•\n\n(æœªåœ¨è¯¥é¡µé¢æ‰¾åˆ°ç›®å½•ä¿¡æ¯)"


    # --- 3. ç»„åˆæˆæœ€ç»ˆçš„Markdownæ–‡ä»¶å†…å®¹ ---
    
    # æ„å»ºYAML Frontmatter
    yaml_header = "---\n"
    yaml_header += f"title: {title}\n"
    if author:
        # æ­£åˆ™è¡¨è¾¾å¼å»é™¤ä½œè€…å›½ç±ä¿¡æ¯å¦‚ "[ç¾] "
        cleaned_author = re.sub(r'^\[.*?\]\s*', '', author)
        yaml_header += f"author: {cleaned_author}\n"
    if subtitle:
        yaml_header += f"subtitle: {subtitle}\n"
    if publisher:
        yaml_header += f"publisher: {publisher}\n"
    if producer:
        yaml_header += f"producer: {producer}\n"
    if original_title:
        yaml_header += f'åŸä½œå: "{original_title}"\n' # å¼•å·é˜²æ­¢ç‰¹æ®Šå­—ç¬¦é—®é¢˜
    if series:
        yaml_header += f"series: {series}\n"
    if publish_date:
        yaml_header += f"publishDate: {publish_date}\n"
    if isbn:
        yaml_header += f'ISBN: "{isbn}"\n'
    if translators:
        yaml_header += "è¯‘è€…:\n"
        for translator in translators:
            yaml_header += f"  - {translator}\n"
    
    yaml_header += "tags:\n  - é˜…è¯»\n"
    yaml_header += f"cover: {cover_url}\n"
    yaml_header += "Finished: false\n"
    yaml_header += "---\n\n"

    # ç»„åˆæœ€ç»ˆå†…å®¹
    final_content = yaml_header + toc_markdown
    
    return title, final_content


if __name__ == "__main__":
    # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å
    input_html_file = 'E:/CODE/Test/Files/douban.html'
    
    try:
        with open(input_html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
            
        title, markdown_content = parse_douban_html(html_content)
        
        if title and markdown_content:
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            safe_title = re.sub(r'[\\/*?:"<>|]', "", title)
            output_md_file = f"E:/CODE/Test/Targets/{safe_title}.md"
            
            with open(output_md_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
                
            print(f"ğŸ‰ æˆåŠŸï¼å·²ä¸ºä½ ç”ŸæˆçŸ¥è¯†å¡ç‰‡ï¼š'{output_md_file}'")

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{input_html_file}'ã€‚è¯·ç¡®ä¿å®ƒå’Œè„šæœ¬åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ï¼š{e}")