def html_table_to_markdown(table):
    """
    å°†bs4çš„<table>èŠ‚ç‚¹è½¬ä¸ºMarkdownè¡¨æ ¼å­—ç¬¦ä¸²
    """
    rows = table.find_all('tr')
    if not rows:
        return ''
    md_lines = []
    # å¤„ç†è¡¨å¤´
    headers = [cell.get_text(strip=True) for cell in rows[0].find_all(['th', 'td'])]
    md_lines.append('| ' + ' | '.join(headers) + ' |')
    md_lines.append('|' + '|'.join([' --- ' for _ in headers]) + '|')
    # å¤„ç†è¡¨ä½“
    for row in rows[1:]:
        cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]
        md_lines.append('| ' + ' | '.join(cells) + ' |')
    return '\n'.join(md_lines)
"""
site: pkulaw.com
    description: è‡ªåŠ¨æ£€æµ‹åŒ—å¤§æ³•å®å¯¼å‡ºçš„HTMLç±»å‹ï¼ˆè®ºæ–‡/æ³•è§„/æ¡ˆä¾‹ï¼‰ï¼Œå¹¶è½¬æ¢ä¸ºPKBè§„èŒƒçš„Markdownæ–‡ä»¶
"""
import os
import re
from bs4 import BeautifulSoup, NavigableString

# --- é…ç½®åŒº ---
INPUT_HTML_FILE = 'E:/CODE/Test/Files/pkulaw.html'  # ä½ è¦å¤„ç†çš„HTMLæ–‡ä»¶
OUTPUT_DIR = 'E:/CODE/Test/Targets/'

# --- è®ºæ–‡å¤„ç†é€»è¾‘ ---
def process_paper(soup):
    metadata = {}
    metadata['title'] = soup.find('h2', class_='title').text.strip()
    safe_title = re.sub(r'[\\/*?:"<>|\n\r\t]', "", metadata['title'])
    safe_title = re.sub(r'\s+', ' ', safe_title).strip()
    output_file = os.path.join(OUTPUT_DIR, f"{safe_title}.md")
    fields_map = {
        'ä½œè€…ï¼š': 'author',
        'æœŸåˆŠåç§°ï¼š': 'journal',
        'æœŸåˆŠå¹´ä»½ï¼š': 'year',
        'æœŸå·ï¼š': 'issue',
        'å…³é”®è¯ï¼š': 'keywords'
    }
    for li in soup.select('.fields li'):
        key_strong = li.find('strong')
        if key_strong:
            key_text = key_strong.text.strip()
            if key_text in fields_map:
                field_name = fields_map[key_text]
                if field_name == 'keywords':
                    metadata[field_name] = [a.text.strip() for a in li.find_all('a')]
                elif field_name == 'journal':
                    box_text = li.find('div', class_='box').decode_contents()
                    match = re.search(r'ã€Š[^ã€Šã€‹]+ã€‹', box_text)
                    if match:
                        value = match.group(0)
                        metadata[field_name] = value
                elif field_name == 'author':
                    value = li.find('div', class_='box').text.replace(key_text, '').strip()
                    authors = re.split(r'[ï¼›;\s]+', value)
                    authors = [a for a in authors if a]
                    if len(authors) > 1:
                        metadata[field_name] = authors
                    else:
                        metadata[field_name] = value
                else:
                    value = li.find('div', class_='box').text.replace(key_text, '').strip()
                    metadata[field_name] = value
    abstract = ""
    strong_abstract = soup.select_one('strong:contains("æ‘˜è¦ï¼š")')
    if strong_abstract:
        abstract_p = strong_abstract.find_next('p')
        if abstract_p:
            abstract = abstract_p.text.strip()
    full_text_div = soup.find('div', id='divFullText')
    footnotes = {}
    footnote_spans = full_text_div.find_all('span', class_='footnote')
    for span in footnote_spans:
        note_id = re.search(r'\[(\d+)\]', span.text)
        if note_id:
            fn_id = note_id.group(1)
            fn_content = span['content'].strip()
            footnotes[fn_id] = fn_content
            span.replace_with(f'[^{fn_id}]')
    main_body_md = []
    heading_patterns = [
        {"pattern": "^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€", "level": "####"},
        {"pattern": "^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰", "level": "#####"},
        {"pattern": "^\\d+\\.", "level": "######"}
    ]
    content_elements = full_text_div.find_all('p', recursive=False)
    for p in content_elements:
        p_text = p.text.strip()
        if not p_text or "ã€æ³¨é‡Šã€‘" in p_text or "ä½œè€…å•ä½ï¼š" in p_text:
            continue
        is_heading = False
        for heading in heading_patterns:
            if re.match(heading['pattern'], p_text):
                clean_title = re.sub(heading['pattern'], '', p_text).strip()
                main_body_md.append(f"{heading['level']} {clean_title}\n")
                is_heading = True
                break
        if not is_heading:
            main_body_md.append(p_text + '\n')
    main_body_str = "\n".join(main_body_md)
    yaml_lines = ['---']
    yaml_lines.append(f"title: \"{metadata.get('title', '')}\"")
    if isinstance(metadata.get('author'), list):
        yaml_lines.append("author:")
        for a in metadata['author']:
            yaml_lines.append(f"  - {a}")
    else:
        yaml_lines.append(f"author: {metadata.get('author', '')}")
    yaml_lines.append(f"journal: {metadata.get('journal', '')}")
    yaml_lines.append(f"year: {metadata.get('year', '')}")
    yaml_lines.append(f"issue: {metadata.get('issue', '')}")
    if metadata.get('keywords'):
        yaml_lines.append("keywords:")
        for kw in metadata['keywords']:
            yaml_lines.append(f"  - {kw}")
    yaml_lines.append("tags:\n  - æ–‡çŒ®/æœŸåˆŠæ–‡ç« ")
    yaml_lines.append("Finished: false")
    yaml_lines.append('---')
    yaml_str = "\n".join(yaml_lines)
    if abstract != "":
        abstract_str = f"> [!abstract]- æ‘˜è¦\n> {abstract}"
    else:
        abstract_str = ""
    footnote_lines = ['---', '### è„šæ³¨\n']
    for fn_id, fn_content in sorted(footnotes.items(), key=lambda item: int(item[0])):
        footnote_lines.append(f"[^{fn_id}]: {fn_content}\n")
    footnote_str = "\n".join(footnote_lines)
    # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
    tables = full_text_div.find_all('table') if full_text_div else []
    table_md = []
    for table in tables:
        md = html_table_to_markdown(table)
        if md:
            table_md.append(md)
    if table_md:
        tables_section = '\n\n---\n\n### é™„è¡¨\n' + '\n\n'.join(table_md)
    else:
        tables_section = ''
    final_md_content = f"{yaml_str}\n\n{abstract_str}\n\n---\n\n#### å‰è¨€\n\n{main_body_str}\n{footnote_str}{tables_section}"
    return final_md_content, output_file

# --- æ³•è§„å¤„ç†é€»è¾‘ ---
def process_regulation(soup):
    # ç›´æ¥å¤ç”¨ regulation_process.py é‡Œçš„é€»è¾‘
    # ...existing code...
    # 1. æå–å…ƒæ•°æ®
    metadata = {}
    title_tag = soup.find('h2', class_='title')
    if title_tag:
        raw_title = ''.join([t for t in title_tag.contents if isinstance(t, NavigableString)]).strip()
        # åªå»é™¤æ–‡ä»¶åä¸­æœ€åä¸€ä¸ªæ‹¬å·åŠå…¶å†…å®¹ï¼ˆåŒ…æ‹¬ä¸­æ–‡å…¨è§’æ‹¬å·å’Œè‹±æ–‡åŠè§’æ‹¬å·ï¼‰ï¼Œä¿ç•™å…¶ä»–æ‹¬å·å†…å®¹
        def remove_last_bracket_content(s):
            import re
            # åŒ¹é…æ‰€æœ‰å…¨è§’æ‹¬å·
            cn = list(re.finditer(r'ï¼ˆ([^ï¼ˆï¼‰]*?)ï¼‰', s))
            # åŒ¹é…æ‰€æœ‰åŠè§’æ‹¬å·
            en = list(re.finditer(r'\(([^()]*)\)', s))
            all_brackets = cn + en
            if not all_brackets:
                return s
            # æ‰¾åˆ°æœ€åä¸€ä¸ªæ‹¬å·å¯¹
            last = max(all_brackets, key=lambda m: m.start())
            content = last.group(1)
            # åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºå•ä¸ªå¤§å†™ä¸­æ–‡æ•°å­—ï¼ˆå¦‚ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‰
            if len(content) == 1 and content in 'ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å':
                return s  # ä¿ç•™è¯¥æ‹¬å·
            return s[:last.start()] + s[last.end():]
        pure_title = remove_last_bracket_content(raw_title).strip()
    else:
        raw_title = pure_title = "æœªçŸ¥æ³•è§„"
    metadata['title'] = raw_title
    def get_field_text_by_label(label_text):
        strong_tag = soup.find('strong', string=lambda text: text and label_text in text)
        if strong_tag:
            parent_box = strong_tag.find_parent(class_='box')
            if parent_box:
                return parent_box.get_text(strip=True).replace(label_text, '').replace('ï¼š', '').strip()
        return None
    tag_map = [
        (['æ³•å¾‹', 'å…¨å›½äººå¤§', 'å¸¸å§”ä¼š'], '\n  - è§„èŒƒ/æ³•å¾‹'),
        (['è¡Œæ”¿æ³•è§„', 'å›½åŠ¡é™¢'], '\n  - è§„èŒƒ/è¡Œæ”¿æ³•è§„'),
        (['å¸æ³•è§£é‡Š'], '\n  - è§„èŒƒ/å¸æ³•è§£é‡Š'),
        (['å¸æ³•è§£é‡Šæ€§è´¨æ–‡ä»¶'], '\n  - è§„èŒƒ/å¸æ³•è§£é‡Š/å¸æ³•è§£é‡Šæ€§è´¨æ–‡ä»¶'),
        (['ä¸¤é«˜å·¥ä½œæ–‡ä»¶'], '\n  - è§„èŒƒ/å¸æ³•è§£é‡Š/ä¸¤é«˜å·¥ä½œæ–‡ä»¶'),
        (['éƒ¨é—¨è§„ç« ', 'éƒ¨å§”'], '\n  - è§„èŒƒ/éƒ¨é—¨è§„ç« '),
        (['éƒ¨é—¨è§„èŒƒæ€§æ–‡ä»¶'],'\n  - è§„èŒƒ/éƒ¨é—¨è§„ç« /éƒ¨é—¨è§„èŒƒæ€§æ–‡ä»¶'),
        (['éƒ¨é—¨å·¥ä½œæ–‡ä»¶'],'\n  - è§„èŒƒ/éƒ¨é—¨è§„ç« /éƒ¨é—¨å·¥ä½œæ–‡ä»¶'),
        (['å›½é™…æ¡çº¦'], '#è§„èŒƒ/å›½é™…æ¡çº¦'),
    ]
    drafting_body_list = []
    strong_tag = soup.find('strong', string=lambda text: text and 'åˆ¶å®šæœºå…³' in text)
    if strong_tag:
        parent_box = strong_tag.find_parent(class_='box')
        if parent_box:
            for span in parent_box.find_all('span', title=True):
                title = span.get('title', '').strip()
                if title:
                    drafting_body_list.append(f"  - {title}")
    if drafting_body_list:
        metadata['åˆ¶å®šæœºå…³'] = '\n' + '\n'.join(drafting_body_list)
    else:
        metadata['åˆ¶å®šæœºå…³'] = None
    metadata['å‘æ–‡å­—å·'] = soup.find('li', class_='row', title=True).get('title') if soup.find('li', class_='row', title=True) else get_field_text_by_label('å‘æ–‡å­—å·')
    metadata['å…¬å¸ƒæ—¥æœŸ'] = get_field_text_by_label('å…¬å¸ƒæ—¥æœŸ')
    metadata['æ–½è¡Œæ—¥æœŸ'] = get_field_text_by_label('æ–½è¡Œæ—¥æœŸ')
    metadata['æ—¶æ•ˆæ€§'] = get_field_text_by_label('æ—¶æ•ˆæ€§')
    eff_level = get_field_text_by_label('æ•ˆåŠ›ä½é˜¶')
    if eff_level:
        for keywords, tag in tag_map:
            if any(k in eff_level for k in keywords):
                metadata['tags'] = tag
                break
    pub_date = metadata.get('å…¬å¸ƒæ—¥æœŸ', '')
    date_str = ''
    if pub_date:
        m = re.search(r'(\d{4})[.\-å¹´](\d{2})[.\-æœˆ](\d{2})', pub_date)
        if m:
            date_str = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
    eff_level = metadata.get('tags', '')
    is_judicial_interpretation = False
    if eff_level and 'å¸æ³•è§£é‡Š' in eff_level:
        is_judicial_interpretation = True
    m = re.search(r'(.*?[é™¢éƒ¨ä¼šå§”å±€å…ç½²])?å…³äº(.*)', pure_title)
    if m:
        short_title = f"å…³äº{m.group(2).strip()}"
        if date_str:
            final_title = f"{date_str} - {short_title}"
        else:
            final_title = short_title
    else:
        if pure_title.startswith("ä¸­åäººæ°‘å…±å’Œå›½"):
            pure_title = pure_title.replace("ä¸­åäººæ°‘å…±å’Œå›½", "").strip()
        if date_str:
            final_title = f"{date_str} - {pure_title}"
        else:
            final_title = pure_title
    metadata['title'] = final_title
    yaml_lines = ["---"]
    for key, value in metadata.items():
        if value and key != 'title':
            yaml_lines.append(f"{key}: {value}")
    yaml_lines.append("---")
    yaml_header = "\n".join(yaml_lines)
    full_text_div = soup.find('div', id='divFullText')
    if full_text_div:
        for a_tag in full_text_div.find_all('a'):
            a_tag.unwrap()
        for fb_dropdown in full_text_div.find_all(class_=['TiaoYinV2', 'c-icon']):
            fb_dropdown.decompose()
        content_parts = []
        has_tiao_wrap = any(
            hasattr(element, 'get') and element.get('class') and 'tiao-wrap' in element.get('class', [])
            for element in full_text_div.children if hasattr(element, 'get')
        )
        if has_tiao_wrap:
            for element in full_text_div.children:
                if isinstance(element, NavigableString):
                    continue
                if not hasattr(element, 'name'):
                    continue
                class_list = element.get('class', []) if element.has_attr('class') else []
                if 'navbian' in class_list:
                    content_parts.append(f"## {element.get_text(strip=True)}\n")
                elif 'navzhang' in class_list:
                    content_parts.append(f"### {element.get_text(strip=True)}\n")
                elif 'navjie' in class_list:
                    content_parts.append(f"#### {element.get_text(strip=True)}\n")
                elif 'tiao-wrap' in class_list:
                    tiao_span = element.find('span', class_='navtiao')
                    if tiao_span:
                        tiao_text = re.sub(r'\s+', ' ', tiao_span.get_text(strip=True)).strip()
                        content_parts.append(f"###### {tiao_text}")
                    for kuan_wrap in element.find_all('div', class_='kuan-wrap'):
                        kuan_texts = []
                        contents = kuan_wrap.find_all(class_=['kuan-content', 'xiang-content'])
                        for content in contents:
                            if content.find('span', class_='navtiao'):
                                content.find('span', class_='navtiao').decompose()
                            cleaned_text = content.get_text().replace('ã€€', '  ').strip()
                            if cleaned_text:
                                kuan_texts.append(cleaned_text)
                        full_kuan_text = "\n".join(kuan_texts)
                        content_parts.append(full_kuan_text)
                    content_parts.append('')
                elif element.name == 'div' and element.get('align') == 'center':
                    content_parts.append(element.get_text(separator='\n', strip=True))
                    content_parts.append('')
                elif element.name == 'p' and element.get_text(strip=True):
                    content_parts.append(element.get_text(strip=True))
                    content_parts.append('')
        else:
            # æ²¡æœ‰ tiao-wrapï¼Œç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ navtiao spanï¼ŒæŒ‰æ¡æ ‡é¢˜å’Œæ­£æ–‡åˆ†ç»„
            navtiao_spans = full_text_div.find_all('span', class_='navtiao')
            for navtiao in navtiao_spans:
                tiao_text = re.sub(r'\s+', ' ', navtiao.get_text(strip=True)).strip()
                content_parts.append(f"###### {tiao_text}")
                # æ”¶é›†è¯¥ span åé¢ç´§è·Ÿçš„æ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª navtiao æˆ–ç»“æŸ
                tiao_content = []
                for sib in navtiao.next_siblings:
                    if getattr(sib, 'name', None) == 'span' and 'navtiao' in sib.get('class', []):
                        break
                    if isinstance(sib, NavigableString):
                        text = sib.strip()
                        if text:
                            tiao_content.append(text)
                    elif hasattr(sib, 'get_text'):
                        text = sib.get_text(strip=True)
                        if text:
                            tiao_content.append(text)
                if tiao_content:
                    content_parts.append("\n".join(tiao_content))
                content_parts.append('')
        main_content = "\n".join(content_parts)
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = full_text_div.find_all('table')
        table_md = []
        for table in tables:
            md = html_table_to_markdown(table)
            if md:
                table_md.append(md)
        if table_md:
            tables_section = '\n\n---\n\n### é™„è¡¨\n' + '\n\n'.join(table_md)
        else:
            tables_section = ''
        main_content = main_content + tables_section
    else:
        main_content = "æœªèƒ½æ‰¾åˆ°æ­£æ–‡å†…å®¹ã€‚"
    main_content = re.sub(r'ã€‚\n(?=.)', 'ã€‚\n\n', main_content)
    final_markdown = f"{yaml_header}\n\n{main_content}"
    safe_title = re.sub(r'[\\/*?:"<>|\n\r\t]', "", final_title)
    safe_title = re.sub(r'\s+', ' ', safe_title).strip()
    output_file = os.path.join(OUTPUT_DIR, f"{safe_title}.md")
    split1 = final_markdown.find('---', final_markdown.find('---')+3)
    if split1 != -1:
        hash_pos = final_markdown.find('#', split1)
        if hash_pos != -1:
            final_markdown = final_markdown[:split1+3] + '\n' + final_markdown[hash_pos:]
    return final_markdown, output_file

# --- æ¡ˆä¾‹å¤„ç†é€»è¾‘ ---
def process_case(soup):
    metadata = {}
    def get_field_text(soup_obj, label_text):
        strong_tag = soup_obj.find('strong', string=lambda text: text and label_text in text)
        if strong_tag:
            parent_box = strong_tag.find_parent(class_='box')
            if parent_box:
                links = parent_box.find_all('a')
                if links:
                    return [a.get_text(strip=True) for a in links]
                else:
                    return [strong_tag.parent.get_text(strip=True).replace(label_text, '').replace('ï¼š', '').strip()]
        return []
    case_no_li = soup.find('li', title=lambda x: x and 'å·' in x)
    if case_no_li:
        metadata['æ¡ˆå·'] = case_no_li.get('title', 'æœªçŸ¥æ¡ˆå·').strip()
    else:
        case_no_span = soup.find('span', class_='case-flag self')
        metadata['æ¡ˆå·'] = case_no_span.get_text(strip=True) if case_no_span else 'æœªçŸ¥æ¡ˆå·'
    metadata['å®¡ç†æ³•é™¢'] = get_field_text(soup, 'å®¡ç†æ³•é™¢')
    metadata['å®¡ç»“æ—¥æœŸ'] = get_field_text(soup, 'å®¡ç»“æ—¥æœŸ')
    metadata['æ–‡ä¹¦ç±»å‹'] = get_field_text(soup, 'æ–‡ä¹¦ç±»å‹')
    metadata['å®¡ç†ç¨‹åº'] = get_field_text(soup, 'å®¡ç†ç¨‹åº')
    metadata['keywords'] = get_field_text(soup, 'æƒè´£å…³é”®è¯')
    metadata['æ¡ˆä»¶è¦ç´ '] = get_field_text(soup, 'æ¡ˆä»¶è¦ç´ ')
    anyou_links = soup.find('strong', string='æ¡ˆç”±ï¼š').parent.find_all('a', class_=None)
    anyou_parts = [a.get_text(strip=True) for a in anyou_links]
    if anyou_parts:
        metadata['æ¡ˆç”±_tag'] = f"æ°‘äº‹æ¡ˆç”±/{'/'.join(anyou_parts[1:])}"
    else:
        metadata['æ¡ˆç”±_tag'] = ""
    yaml_lines = ["---"]
    for key, values in metadata.items():
        if not values: continue
        if key.endswith('_tag'):
            yaml_lines.append("tags:")
            yaml_lines.append(f"  - {values}")
        elif isinstance(values, list):
            yaml_lines.append(f"{key}:")
            for value in values:
                yaml_lines.append(f"  - {value}")
        else:
            yaml_lines.append(f"{key}: {values}")
    yaml_lines.append("---")
    yaml_header = "\n".join(yaml_lines)
    full_text_div = soup.find('div', id='divFullText')
    if full_text_div:
        a_tags_to_merge = full_text_div.find_all('a')
        for a_tag in a_tags_to_merge:
            prev_sibling = a_tag.previous_sibling
            next_sibling = a_tag.next_sibling
            a_text = a_tag.get_text()
            if prev_sibling and isinstance(prev_sibling, NavigableString):
                combined_text = prev_sibling.string.rstrip() + a_text
                if next_sibling and isinstance(next_sibling, NavigableString):
                    combined_text += next_sibling.string.lstrip()
                    next_sibling.extract()
                prev_sibling.string.replace_with(combined_text)
                a_tag.extract()
            else:
                a_tag.replace_with(a_text)
        spans_to_merge = full_text_div.find_all('span', class_=lambda c: c and 'case-flag' in c)
        for span in spans_to_merge:
            prev_sibling = span.previous_sibling
            next_sibling = span.next_sibling
            span_text = span.get_text()
            if prev_sibling and isinstance(prev_sibling, NavigableString):
                combined_text = prev_sibling.string.rstrip() + span_text
                if next_sibling and isinstance(next_sibling, NavigableString):
                    combined_text += next_sibling.string.lstrip()
                    next_sibling.extract()
                prev_sibling.string.replace_with(combined_text)
                span.extract()
            else:
                span.replace_with(span_text)
        for span_tag in full_text_div.find_all('span', class_='anchor-case'):
            title_text = span_tag.get_text(strip=True)
            if title_text:
                span_tag.insert_before(f"\n\n##### {title_text}\n")
            span_tag.decompose()
        main_content = full_text_div.get_text(separator='\n\n', strip=True)
        main_content = main_content.replace('[', r'\[').replace(']', r'\]')
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = full_text_div.find_all('table')
        table_md = []
        for table in tables:
            md = html_table_to_markdown(table)
            if md:
                table_md.append(md)
        if table_md:
            tables_section = '\n\n---\n\n### é™„è¡¨\n' + '\n\n'.join(table_md)
        else:
            tables_section = ''
        main_content = main_content + tables_section
    else:
        main_content = "æœªèƒ½æ‰¾åˆ°æ­£æ–‡å†…å®¹ã€‚"
    final_markdown = f"{yaml_header}\n\n{main_content}"
    safe_case_title = re.sub(r'[\\/*?:"<>|\n\r\t]', "", metadata.get('æ¡ˆå·', 'æœªå‘½åæ¡ˆä¾‹'))
    safe_case_title = re.sub(r'\s+', ' ', safe_case_title).strip()
    output_file = os.path.join(OUTPUT_DIR, f"{safe_case_title}.md")
    return final_markdown, output_file

import re
# --- ç±»å‹è‡ªåŠ¨æ£€æµ‹ä¸ä¸»æµç¨‹ ---
def detect_type_and_process(html_filepath):
    with open(html_filepath, 'r', encoding='utf-8') as f:
        html_content = f.read()
    soup = BeautifulSoup(html_content, 'lxml')
    # ä¼˜å…ˆé€šè¿‡æ³•å®å¼•è¯ç åˆ¤æ–­ç±»å‹
    citation_code = None
    # å¸¸è§å¼•è¯ç æ ¼å¼ï¼šCLI.1.153700ã€CLI.11.518085ã€CLI.WR.3553ã€CLI.C.375295ç­‰
    code_match = re.search(r'(CLI\.[A-Z0-9]+\.[A-Z0-9]+|CLI\.[A-Z]+\.[A-Z0-9]+|CLI\.[A-Z]+\.[A-Z0-9]+|CLI\.[A-Z]+)', html_content)
    if code_match:
        citation_code = code_match.group(0)
        # æ³•å¾‹æ³•è§„ï¼ˆä¸­å¤®/åœ°æ–¹æ³•è§„/ä¸­å¤–æ¡çº¦/å¤–å›½/æ¸¯æ¾³å°/å¹´é‰´/è‹±æ–‡è¯‘æœ¬ç­‰ï¼‰
        if re.match(r'CLI\.(1|2|3|11|T|FL|HK|MAC|TW|WR|N|ALE)\.', citation_code) or citation_code.startswith('CLI.WR.'):
            print(f'æ£€æµ‹åˆ°ç±»å‹ï¼šæ³•è§„ï¼ˆå¼•è¯ç  {citation_code}ï¼‰')
            return process_regulation(soup)
        # æ¡ˆä¾‹/åˆ¤å†³/ä»²è£/æ¡ˆä¾‹æŠ¥é“/æ£€å¯Ÿæ–‡ä¹¦/è¡Œæ”¿æ‰§æ³•/åˆåŒèŒƒæœ¬/æ³•å¾‹æ–‡ä¹¦
        elif re.match(r'CLI\.(C|CR|AA|P|LD|ALE|CS)\.', citation_code):
            print(f'æ£€æµ‹åˆ°ç±»å‹ï¼šæ¡ˆä¾‹ï¼ˆå¼•è¯ç  {citation_code}ï¼‰')
            return process_case(soup)
        # æœŸåˆŠ/æ–‡çŒ®/ä¸“å®¶è§£è¯»/å¾‹æ‰€å®åŠ¡/æ³•å­¦æœŸåˆŠ/æ³•å­¦æ–‡çŒ®
        elif re.match(r'CLI\.(A|J|L|A)\.', citation_code):
            print(f'æ£€æµ‹åˆ°ç±»å‹ï¼šè®ºæ–‡/æ–‡çŒ®ï¼ˆå¼•è¯ç  {citation_code}ï¼‰')
            return process_paper(soup)
    # å¦‚æœå¼•è¯ç æœªå‘½ä¸­ï¼Œå›é€€HTMLç»“æ„åˆ¤æ–­
    # æ£€æµ‹è®ºæ–‡
    if soup.find('h2', class_='title') and soup.find('div', class_='fields'):
        print('æ£€æµ‹åˆ°ç±»å‹ï¼šè®ºæ–‡')
        return process_paper(soup)
    # æ£€æµ‹æ³•è§„
    if soup.find('h2', class_='title') and soup.find('div', id='divFullText') and soup.find('strong', string=lambda t: t and 'åˆ¶å®šæœºå…³' in t):
        print('æ£€æµ‹åˆ°ç±»å‹ï¼šæ³•è§„')
        return process_regulation(soup)
    # æ£€æµ‹æ¡ˆä¾‹
    if soup.find('h2', class_='title') and soup.find('div', id='divFullText') and soup.find('strong', string=lambda t: t and 'å®¡ç†æ³•é™¢' in t):
        print('æ£€æµ‹åˆ°ç±»å‹ï¼šæ¡ˆä¾‹')
        return process_case(soup)
    print('æœªèƒ½è¯†åˆ«HTMLç±»å‹ï¼Œæœªå¤„ç†ã€‚')
    return None, None

if __name__ == "__main__":
    if not os.path.exists(INPUT_HTML_FILE):
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{INPUT_HTML_FILE}' ä¸å­˜åœ¨ã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶å’Œè„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
    else:
        markdown_output, output_filename = detect_type_and_process(INPUT_HTML_FILE)
        if markdown_output and output_filename:
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(markdown_output)
            print(f"ğŸ‰ æˆåŠŸï¼å·²å°†å†…å®¹è§£æå¹¶ä¿å­˜ä¸º: {output_filename}")
        else:
            print("æœªç”Ÿæˆä»»ä½•è¾“å‡ºã€‚")
